{
  "gpt_6b": "GPT-J is a language model with 6 billion parameters trained on the Pile (825 GB of text data) published by EleutherAI. ",
  "boris_6B": "Boris is a fine tuned version of GPT-J for the French language. ",
  "fairseq_gpt_13B": "Fairseq GPT 13B is an English language model with 13 billion parameters",
  "gptneox_20B": "GPT-NeoX-20B is the largest publically available English language model with 20 billion parameters"
} 
